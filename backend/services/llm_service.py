from __future__ import annotations

import asyncio
import json
import re
from collections.abc import Sequence
from typing import Any

from core import get_settings
from schemas import (
    CrawlItem,
    LLMBatchItemAnalysisResult,
    LLMInsightItem,
    LLMItemAnalysisResult,
    LLMSummaryResult,
)

# åŠ¨æ€å¯¼å…¥ SDKï¼Œé¿å…å› ç¼ºå°‘ä¾èµ–å¯¼è‡´æ•´ä¸ªæœåŠ¡å´©æºƒ
try:
    from zai import ZhipuAiClient
except Exception:  # pragma: no cover
    ZhipuAiClient = None  # type: ignore[assignment]


class LLMService:
    """LLM æœåŠ¡ï¼šæ„å»º Promptã€è°ƒç”¨ GLMã€æ ¡éªŒè¾“å‡ºæ ¼å¼ã€‚
    
    ç±»ä¼¼ Java çš„ Service å±‚ï¼Œå°è£…äº†å¯¹å¤§æ¨¡å‹çš„è°ƒç”¨é€»è¾‘ã€‚
    """

    def __init__(self) -> None:
        self._settings = get_settings()
        # æ„é€ å‡½æ•°ä¸­æ£€æŸ¥ä¾èµ–
        if ZhipuAiClient is None:
            raise RuntimeError("æœªå®‰è£… zai-sdkï¼Œè¯·å…ˆæ‰§è¡Œ `python3 -m uv sync`ã€‚")

    def build_messages(self, items: Sequence[CrawlItem]) -> list[dict[str, str]]:
        """
        æ„å»ºèŠå¤©æ¶ˆæ¯ï¼ˆPromptï¼‰ã€‚
        
        Args:
            items: æŠ“å–åˆ°çš„æ¨æ–‡åˆ—è¡¨
            
        Returns:
            ç¬¦åˆ OpenAI/GLM æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
        """
        lines: list[str] = []
        # enumerate(items, start=1): å¸¦ç´¢å¼•éå†ï¼Œç´¢å¼•ä» 1 å¼€å§‹
        for index, item in enumerate(items, start=1):
            lines.append(
                f"{index}. [{item.author_username}] {item.text}\n"
                f"   - url: {item.url}\n"
                f"   - published_at: {item.published_at}"
            )

        # Prompt Engineering (æç¤ºè¯å·¥ç¨‹)
        # è¿™é‡Œä½¿ç”¨äº† Few-Shot Prompting æˆ– ç»“æ„åŒ–è¾“å‡ºæŒ‡ä»¤
        content = (
            "è¯·åŸºäºä»¥ä¸‹æ¨æ–‡å†…å®¹ï¼Œè¾“å‡ºæ¯ä¸€ä»½æŠ€æœ¯èµ„è®¯æ´å¯Ÿã€‚\n"
            "ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ä¸‹é¢ Markdown æ¨¡æ¿è¾“å‡ºï¼ˆä¸è¦åŠ å…¶ä»–è§£é‡Šï¼‰ï¼š\n\n"
            "## ğŸ“Š AIåˆ†ææ€»è§ˆ\n"
            "- ç»¼åˆè¯„åˆ†: <0-100çš„æ•´æ•°>\n"
            "- æ•°æ®é‡: <æ•°å­—>\n\n"
            "## ğŸ” å…³é”®æ´å¯Ÿ\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n\n"
            "è¦æ±‚ï¼š\n"
            "1) AIè¯„åˆ†ä»£è¡¨æŠ€æœ¯ä»·å€¼ä¸å¯æ‰§è¡Œæ€§ã€‚\n"
            "2) è§‚ç‚¹å¿…é¡»å¯è½åœ°ï¼Œä¸è¦ç©ºæ³›ã€‚\n"
            "3) åªä½¿ç”¨è¾“å…¥é‡Œå­˜åœ¨çš„ IDã€‚\n\n"
            f"è¾“å…¥æ•°æ®ï¼š\n{chr(10).join(lines)}"
        )

        return [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æŠ€æœ¯æƒ…æŠ¥åˆ†æåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": content},
        ]

    async def summarize(self, items: Sequence[CrawlItem]) -> LLMSummaryResult:
        """
        æ ¸å¿ƒä¸šåŠ¡æ–¹æ³•ï¼šç”Ÿæˆå†…å®¹æ€»ç»“ã€‚
        
        æµç¨‹ï¼š
        1. æ ¡éªŒè¾“å…¥
        2. æ„å»º Prompt
        3. è°ƒç”¨å¤§æ¨¡å‹ (å¸¦é‡è¯•æœºåˆ¶)
        4. è§£æè¿”å›ç»“æœ (Markdown -> å¯¹è±¡)
        """
        if not items:
            # å¿«é€Ÿå¤±è´¥ (Fast Return)
            return LLMSummaryResult(
                status="degraded",
                summary_markdown="- æœ‰æ•ˆä¿¡æ¯ä¸è¶³ï¼Œæœªç”Ÿæˆæ€»ç»“ã€‚",
                highlights=[],
                model=self._settings.GLM_MODEL,
                failure_reason="no_input_items",
            )

        api_key = self._settings.ZAI_API_KEY
        messages = self.build_messages(items)
        prompt_text = self._messages_to_text(messages)
        
        if not api_key:
            return LLMSummaryResult(
                status="failed",
                summary_markdown="",
                highlights=[],
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                failure_reason="missing_zai_api_key",
            )

        last_error: str | None = None
        last_raw_response: str | None = None
        
        # é‡è¯•å¾ªç¯ (Retry Loop)
        for attempt in range(self._settings.GLM_MAX_RETRIES + 1):
            try:
                # await: å¼‚æ­¥è°ƒç”¨ï¼Œä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹
                raw_content = await self._call_glm(api_key=api_key, messages=messages)
                last_raw_response = raw_content
                
                # è§£æ Markdown ç»“æœ
                parsed = self._parse_summary_response(raw_content)
                parsed.model = self._settings.GLM_MODEL
                parsed.prompt_text = prompt_text
                parsed.raw_response_text = raw_content
                return parsed
                
            except Exception as e:
                last_error = str(e)
                # ç®€å•çš„æŒ‡æ•°é€€é¿ (Exponential Backoff) å¯ä»¥åŠ åœ¨è¿™é‡Œ
                await asyncio.sleep(1) 

        # é‡è¯•è€—å°½ï¼Œè¿”å›å¤±è´¥ç»“æœ
        return LLMSummaryResult(
            status="failed",
            summary_markdown="",
            highlights=[],
            model=self._settings.GLM_MODEL,
            prompt_text=prompt_text,
            raw_response_text=last_raw_response,
            failure_reason=f"max_retries_exceeded: {last_error}",
        )
    
    # ç§æœ‰æ–¹æ³• (Private Methods) ä»¥ _ å¼€å¤´
    # Python æ²¡æœ‰ private å…³é”®å­—ï¼Œè¿™æ˜¯ä¸€ç§çº¦å®š
    def _messages_to_text(self, messages: list[dict[str, str]]) -> str:
        return json.dumps(messages, ensure_ascii=False)

    async def _call_glm(self, api_key: str, messages: list[dict[str, str]]) -> str:
        """è°ƒç”¨æ™ºè°± GLM API çš„åº•å±‚å®ç°ã€‚"""
        # å®ä¾‹åŒ–å®¢æˆ·ç«¯
        client = ZhipuAiClient(api_key=api_key)
        
        # è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­ï¼Œå› ä¸º ZhipuAiClient å¯èƒ½æ˜¯åŒæ­¥çš„åº“
        # asyncio.to_thread æ˜¯ Python 3.9+ çš„ç‰¹æ€§ï¼Œç”¨äºæŠŠåŒæ­¥é˜»å¡ä»£ç æ”¾åˆ°å¼‚æ­¥çº¿ç¨‹æ± è¿è¡Œ
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model=self._settings.GLM_MODEL,
            messages=messages,
            stream=False,
            temperature=0.1, # ä½æ¸©åº¦ï¼Œè®©å›ç­”æ›´ç¡®å®šã€æ›´ä¸¥è°¨
        )
        return response.choices[0].message.content or ""

    def _parse_summary_response(self, text: str) -> LLMSummaryResult:
        """
        è§£æå™¨ï¼šæŠŠå¤§æ¨¡å‹è¿”å›çš„éç»“æ„åŒ– Markdown æ–‡æœ¬ï¼Œ
        é€šè¿‡æ­£åˆ™ (Regex) æå–ä¸ºç»“æ„åŒ–çš„ LLMSummaryResult å¯¹è±¡ã€‚
        """
        # 1. æå–ç»¼åˆè¯„åˆ†
        score_match = re.search(r"ç»¼åˆè¯„åˆ†[:ï¼š]\s*(\d+)", text)
        overall_score = int(score_match.group(1)) if score_match else 0

        # 2. æå–æ¯æ¡æ´å¯Ÿ (Insight)
        insights: list[LLMInsightItem] = []
        # æ­£åˆ™è§£é‡Šï¼š
        # ID:\s*(.*?): åŒ¹é… ID: åçš„å†…å®¹ï¼Œéè´ªå©ªåŒ¹é…
        # AIè¯„åˆ†:\s*(\d+): åŒ¹é…åˆ†æ•°
        # è§‚ç‚¹:\s*(.*): åŒ¹é…è§‚ç‚¹å†…å®¹
        pattern = re.compile(r"ID:\s*(.*?)\s*\|\s*AIè¯„åˆ†:\s*(\d+)\s*\|\s*è§‚ç‚¹:\s*(.*)")
        
        for line in text.split("\n"):
            line = line.strip()
            if not line.startswith("-"):
                continue
                
            match = pattern.search(line)
            if match:
                insights.append(
                    LLMInsightItem(
                        tweet_id=match.group(1).strip(),
                        ai_score=int(match.group(2)),
                        summary=match.group(3).strip(),
                    )
                )

        return LLMSummaryResult(
            status="success",
            summary_markdown=text,
            overall_score=overall_score,
            insights=insights,
        )
