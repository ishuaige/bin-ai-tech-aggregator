from __future__ import annotations

import asyncio
import json
import re
from collections.abc import Sequence
from typing import Any

from core import get_settings
from schemas import (
    CrawlItem,
    LLMBatchItemAnalysisResult,
    LLMInsightItem,
    LLMItemAnalysisResult,
    LLMSummaryResult,
)

try:
    from zai import ZhipuAiClient
except Exception:  # pragma: no cover
    ZhipuAiClient = None  # type: ignore[assignment]


class LLMService:
    """LLM æœåŠ¡ï¼šæ„å»º Promptã€è°ƒç”¨ GLMã€æ ¡éªŒè¾“å‡ºæ ¼å¼ã€‚"""

    def __init__(self) -> None:
        self._settings = get_settings()
        if ZhipuAiClient is None:
            raise RuntimeError("æœªå®‰è£… zai-sdkï¼Œè¯·å…ˆæ‰§è¡Œ `python3 -m uv sync`ã€‚")

    def build_messages(self, items: Sequence[CrawlItem]) -> list[dict[str, str]]:
        """
        æ„å»ºèŠå¤©æ¶ˆæ¯ï¼ˆPromptï¼‰ã€‚
        ç›®æ ‡ï¼šè¾“å‡º 3-5 æ¡ä¸­æ–‡ Markdown è¦ç‚¹ï¼Œä¾¿äºåç»­æ¨é€ã€‚
        """
        lines: list[str] = []
        for index, item in enumerate(items, start=1):
            lines.append(
                f"{index}. [{item.author_username}] {item.text}\n"
                f"   - url: {item.url}\n"
                f"   - published_at: {item.published_at}"
            )

        content = (
            "è¯·åŸºäºä»¥ä¸‹æ¨æ–‡å†…å®¹ï¼Œè¾“å‡ºæ¯ä¸€ä»½æŠ€æœ¯èµ„è®¯æ´å¯Ÿã€‚\n"
            "ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ä¸‹é¢ Markdown æ¨¡æ¿è¾“å‡ºï¼ˆä¸è¦åŠ å…¶ä»–è§£é‡Šï¼‰ï¼š\n\n"
            "## ğŸ“Š AIåˆ†ææ€»è§ˆ\n"
            "- ç»¼åˆè¯„åˆ†: <0-100çš„æ•´æ•°>\n"
            "- æ•°æ®é‡: <æ•°å­—>\n\n"
            "## ğŸ” å…³é”®æ´å¯Ÿ\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n"
            "- ID: <tweet_id> | AIè¯„åˆ†: <0-100æ•´æ•°> | è§‚ç‚¹: <20-60å­—ï¼Œä¸­æ–‡>\n\n"
            "è¦æ±‚ï¼š\n"
            "1) AIè¯„åˆ†ä»£è¡¨æŠ€æœ¯ä»·å€¼ä¸å¯æ‰§è¡Œæ€§ã€‚\n"
            "2) è§‚ç‚¹å¿…é¡»å¯è½åœ°ï¼Œä¸è¦ç©ºæ³›ã€‚\n"
            "3) åªä½¿ç”¨è¾“å…¥é‡Œå­˜åœ¨çš„ IDã€‚\n\n"
            f"è¾“å…¥æ•°æ®ï¼š\n{chr(10).join(lines)}"
        )

        return [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æŠ€æœ¯æƒ…æŠ¥åˆ†æåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": content},
        ]

    async def summarize(self, items: Sequence[CrawlItem]) -> LLMSummaryResult:
        """
        3.5~3.7 ä¸»å…¥å£ï¼š
        - 3.5 æ„å»º Prompt
        - 3.6 è°ƒç”¨ GLMï¼ˆè¶…æ—¶ + é‡è¯•ï¼‰
        - 3.7 è§£æå¹¶æ ¡éªŒè¿”å›æ ¼å¼ï¼Œå¤±è´¥æ—¶é™çº§
        """
        if not items:
            return LLMSummaryResult(
                status="degraded",
                summary_markdown="- æœ‰æ•ˆä¿¡æ¯ä¸è¶³ï¼Œæœªç”Ÿæˆæ€»ç»“ã€‚",
                highlights=[],
                model=self._settings.GLM_MODEL,
                failure_reason="no_input_items",
            )

        api_key = self._settings.ZAI_API_KEY
        messages = self.build_messages(items)
        prompt_text = self._messages_to_text(messages)
        if not api_key:
            return LLMSummaryResult(
                status="failed",
                summary_markdown="",
                highlights=[],
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                failure_reason="missing_zai_api_key",
            )

        last_error: str | None = None
        last_raw_response: str | None = None
        for attempt in range(self._settings.GLM_MAX_RETRIES + 1):
            try:
                raw_content = await self._call_glm(api_key=api_key, messages=messages)
                last_raw_response = raw_content
                result = self._validate_summary(raw_content)
                return result.model_copy(
                    update={
                        "model": self._settings.GLM_MODEL,
                        "prompt_text": prompt_text,
                        "raw_response_text": last_raw_response,
                    }
                )
            except Exception as exc:  # noqa: BLE001
                last_error = str(exc)
                if attempt >= self._settings.GLM_MAX_RETRIES:
                    break
                # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s...
                await asyncio.sleep(2**attempt)

        return LLMSummaryResult(
            status="failed",
            summary_markdown="",
            highlights=[],
            model=self._settings.GLM_MODEL,
            prompt_text=prompt_text,
            raw_response_text=last_raw_response,
            failure_reason=f"glm_request_failed: {last_error}",
        )

    async def analyze_item(self, item: CrawlItem) -> LLMItemAnalysisResult:
        """
        å•æ¡èµ„è®¯åˆ†æï¼ˆç”¨äºç¼“å­˜ç¼ºå¤±è¡¥é½ï¼‰ã€‚
        åªè¿”å› AIè¯„åˆ† + è§‚ç‚¹ï¼Œä¸è®©æ¨¡å‹ç”Ÿæˆæ¥æº/çƒ­åº¦ç­‰åŸºç¡€å­—æ®µã€‚
        """
        api_key = self._settings.ZAI_API_KEY
        messages = self._build_single_item_messages(item)
        prompt_text = self._messages_to_text(messages)
        if not api_key:
            return LLMItemAnalysisResult(
                status="failed",
                insight=None,
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                failure_reason="missing_zai_api_key",
            )
        try:
            raw_content = await self._call_glm(api_key=api_key, messages=messages)
            score, summary = self._parse_single_item_output(raw_content)
            return LLMItemAnalysisResult(
                status="success",
                insight=LLMInsightItem(tweet_id=item.tweet_id, ai_score=score, summary=summary),
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                raw_response_text=raw_content,
                failure_reason=None,
            )
        except Exception as exc:  # noqa: BLE001
            return LLMItemAnalysisResult(
                status="failed",
                insight=None,
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                raw_response_text=None,
                failure_reason=str(exc),
            )

    async def analyze_items(self, items: Sequence[CrawlItem]) -> LLMBatchItemAnalysisResult:
        """
        æ‰¹é‡åˆ†æå¤šæ¡èµ„è®¯ï¼Œå‡å°‘ API å¾€è¿”æ¬¡æ•°ã€‚
        è¾“å‡ºè¦æ±‚ï¼šJSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« tweet_id/ai_score/summaryã€‚
        """
        if not items:
            return LLMBatchItemAnalysisResult(
                status="degraded",
                insights=[],
                model=self._settings.GLM_MODEL,
                failure_reason="no_input_items",
            )

        api_key = self._settings.ZAI_API_KEY
        messages = self._build_batch_item_messages(items)
        prompt_text = self._messages_to_text(messages)
        if not api_key:
            return LLMBatchItemAnalysisResult(
                status="failed",
                insights=[],
                model=self._settings.GLM_MODEL,
                prompt_text=prompt_text,
                failure_reason="missing_zai_api_key",
            )

        last_error: str | None = None
        last_raw_response: str | None = None
        for attempt in range(self._settings.GLM_MAX_RETRIES + 1):
            try:
                raw_content = await self._call_glm(api_key=api_key, messages=messages)
                last_raw_response = raw_content
                insights = self._parse_batch_item_output(raw_content, allowed_ids={item.tweet_id for item in items})
                status = "success" if insights else "degraded"
                return LLMBatchItemAnalysisResult(
                    status=status,
                    insights=insights,
                    model=self._settings.GLM_MODEL,
                    prompt_text=prompt_text,
                    raw_response_text=last_raw_response,
                    failure_reason=None if insights else "empty_batch_result",
                )
            except Exception as exc:  # noqa: BLE001
                last_error = str(exc)
                if attempt >= self._settings.GLM_MAX_RETRIES:
                    break
                await asyncio.sleep(2**attempt)

        return LLMBatchItemAnalysisResult(
            status="failed",
            insights=[],
            model=self._settings.GLM_MODEL,
            prompt_text=prompt_text,
            raw_response_text=last_raw_response,
            failure_reason=f"glm_batch_request_failed: {last_error}",
        )

    @staticmethod
    def _build_single_item_messages(item: CrawlItem) -> list[dict[str, str]]:
        return [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æŠ€æœ¯åˆ†æåŠ©æ‰‹ã€‚"},
            {
                "role": "user",
                "content": (
                    "è¯·åˆ†æä¸‹é¢è¿™æ¡èµ„è®¯ï¼Œåªè¾“å‡ºä¸¤è¡Œï¼š\n"
                    "AIè¯„åˆ†: <0-100æ•´æ•°>\n"
                    "è§‚ç‚¹: <20-60å­—>\n\n"
                    f"tweet_id: {item.tweet_id}\n"
                    f"ä½œè€…: {item.author_username}\n"
                    f"æ­£æ–‡: {item.text}\n"
                    f"é“¾æ¥: {item.url}"
                ),
            },
        ]

    @staticmethod
    def _build_batch_item_messages(items: Sequence[CrawlItem]) -> list[dict[str, str]]:
        item_lines = []
        for idx, item in enumerate(items, start=1):
            item_lines.append(
                f"{idx}. tweet_id={item.tweet_id}\n"
                f"   author={item.author_username}\n"
                f"   text={item.text}\n"
                f"   url={item.url}"
            )
        content = (
            "è¯·åˆ†æä¸‹é¢å¤šæ¡èµ„è®¯ï¼Œè¾“å‡ºä¸¥æ ¼ JSON æ•°ç»„ï¼Œä¸è¦åŠ  markdown ä»£ç å—ï¼Œä¸è¦åŠ è§£é‡Šã€‚\n"
            "JSON æ¯ä¸ªå…ƒç´ å¿…é¡»åŒ…å«å­—æ®µï¼štweet_id, ai_score, summaryã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1) tweet_id å¿…é¡»æ¥è‡ªè¾“å…¥ã€‚\n"
            "2) ai_score ä¸º 0-100 æ•´æ•°ã€‚\n"
            "3) summary ä¸º 20-60 å­—ä¸­æ–‡ï¼Œå¯æ‰§è¡Œã€é¿å…ç©ºè¯ã€‚\n\n"
            f"è¾“å…¥ï¼š\n{chr(10).join(item_lines)}"
        )
        return [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æŠ€æœ¯åˆ†æåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": content},
        ]

    async def _call_glm(self, api_key: str, messages: list[dict[str, str]]) -> str:
        """åœ¨å­çº¿ç¨‹é‡Œæ‰§è¡Œ SDK åŒæ­¥è°ƒç”¨ï¼Œé¿å…é˜»å¡ async äº‹ä»¶å¾ªç¯ã€‚"""
        client = ZhipuAiClient(api_key=api_key)

        def _sync_call() -> Any:
            return client.chat.completions.create(
                model=self._settings.GLM_MODEL,
                messages=messages,
                temperature=0.3,
            )

        response = await asyncio.wait_for(
            asyncio.to_thread(_sync_call),
            timeout=self._settings.GLM_TIMEOUT_SECONDS,
        )
        content = self._extract_content(response)
        if not content.strip():
            raise ValueError("glm_empty_response")
        return content

    @staticmethod
    def _extract_content(response: Any) -> str:
        """
        å…¼å®¹ SDK çš„å¯¹è±¡/å­—å…¸ç»“æ„ï¼Œå°½é‡å–åˆ° choices[0].message.contentã€‚
        """
        if response is None:
            return ""

        # å¯¹è±¡é£æ ¼ï¼šresponse.choices[0].message.content
        choices = getattr(response, "choices", None)
        if choices and len(choices) > 0:
            first = choices[0]
            message = getattr(first, "message", None)
            content = getattr(message, "content", None)
            if isinstance(content, str):
                return content

        # å­—å…¸é£æ ¼ï¼šresponse["choices"][0]["message"]["content"]
        if isinstance(response, dict):
            r_choices = response.get("choices")
            if isinstance(r_choices, list) and r_choices:
                first = r_choices[0]
                if isinstance(first, dict):
                    message = first.get("message")
                    if isinstance(message, dict) and isinstance(message.get("content"), str):
                        return message["content"]
        return ""

    def _validate_summary(self, summary_markdown: str) -> LLMSummaryResult:
        """
        è§£æå¹¶æ ¡éªŒè¾“å‡ºï¼š
        - å¿…é¡»è‡³å°‘åŒ…å« 3 æ¡ Markdown åˆ—è¡¨é¡¹
        - ä¸æ»¡è¶³æ—¶é™çº§è¿”å›ï¼Œå¹¶å¸¦å¤±è´¥åŸå› 
        """
        lines = [line.strip() for line in summary_markdown.splitlines() if line.strip()]
        highlights = [line[2:].strip() for line in lines if line.startswith("- ") and len(line) > 2]
        overall_score = self._extract_overall_score(lines)
        insights = self._extract_insights(lines)

        if len(highlights) < 3 or len(insights) < 3:
            return LLMSummaryResult(
                status="degraded",
                summary_markdown=summary_markdown,
                highlights=highlights,
                overall_score=overall_score,
                insights=insights,
                failure_reason="invalid_summary_format_or_too_few_points",
            )

        return LLMSummaryResult(
            status="success",
            summary_markdown=summary_markdown,
            highlights=highlights[:5],
            overall_score=overall_score,
            insights=insights[:5],
            failure_reason=None,
        )

    @staticmethod
    def _messages_to_text(messages: list[dict[str, str]]) -> str:
        blocks: list[str] = []
        for message in messages:
            role = message.get("role", "unknown")
            content = message.get("content", "")
            blocks.append(f"[{role}]\n{content}")
        return "\n\n".join(blocks)

    @staticmethod
    def _parse_single_item_output(content: str) -> tuple[int, str]:
        score_match = re.search(r"AIè¯„åˆ†[:ï¼š]\s*(\d{1,3})", content)
        summary_match = re.search(r"è§‚ç‚¹[:ï¼š]\s*(.+)", content)
        score = int(score_match.group(1)) if score_match else 60
        score = max(0, min(100, score))
        summary = summary_match.group(1).strip() if summary_match else content.strip()
        if len(summary) > 120:
            summary = f"{summary[:120]}..."
        return score, summary

    @staticmethod
    def _parse_batch_item_output(content: str, allowed_ids: set[str]) -> list[LLMInsightItem]:
        cleaned = content.strip()
        if "```" in cleaned:
            cleaned = cleaned.replace("```json", "").replace("```", "").strip()

        start = cleaned.find("[")
        end = cleaned.rfind("]")
        if start >= 0 and end > start:
            cleaned = cleaned[start : end + 1]

        payload = json.loads(cleaned)
        if not isinstance(payload, list):
            return []

        results: list[LLMInsightItem] = []
        for row in payload:
            if not isinstance(row, dict):
                continue
            tweet_id = str(row.get("tweet_id") or "").strip()
            if not tweet_id or tweet_id not in allowed_ids:
                continue
            score = int(row.get("ai_score") or 0)
            score = max(0, min(100, score))
            summary = str(row.get("summary") or "").strip()
            if not summary:
                continue
            if len(summary) > 120:
                summary = f"{summary[:120]}..."
            results.append(
                LLMInsightItem(
                    tweet_id=tweet_id,
                    ai_score=score,
                    summary=summary,
                )
            )

        dedup: dict[str, LLMInsightItem] = {}
        for item in results:
            dedup[item.tweet_id] = item
        return list(dedup.values())

    @staticmethod
    def _extract_overall_score(lines: list[str]) -> int | None:
        pattern = re.compile(r"ç»¼åˆè¯„åˆ†[:ï¼š]\s*(\d{1,3})")
        for line in lines:
            match = pattern.search(line)
            if match:
                value = int(match.group(1))
                return max(0, min(100, value))
        return None

    @staticmethod
    def _extract_insights(lines: list[str]) -> list[LLMInsightItem]:
        """
        è§£æå½¢å¦‚ï¼š
        - æ¥æº: xxx | çƒ­åº¦: 82 | AIè¯„åˆ†: 90 | è§‚ç‚¹: xxx
        """
        result: list[LLMInsightItem] = []
        pattern = re.compile(
            r"^-+\s*ID[:ï¼š]\s*(?P<tweet_id>[^|]+)\|\s*AIè¯„åˆ†[:ï¼š]\s*(?P<score>\d{1,3})\s*\|\s*è§‚ç‚¹[:ï¼š]\s*(?P<summary>.+)$"
        )
        for line in lines:
            match = pattern.match(line)
            if not match:
                continue
            score = max(0, min(100, int(match.group("score"))))
            result.append(
                LLMInsightItem(
                    tweet_id=match.group("tweet_id").strip(),
                    ai_score=score,
                    summary=match.group("summary").strip(),
                )
            )
        return result
